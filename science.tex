
% to-do
% -----
% - Read for anonymity!!
% - end-to-end simulation.
% - stellar jitter. in both wobble and in the simulation.
% - Earth-like planets
% - multiple systems?
% - Explicitly reference the Gaps document from JPL.

\documentclass[12pt]{article}
\setlength{\headheight}{2ex} % must be *before* \input{hogg_nasa}
\usepackage{amsmath}
% \usepackage{physics}
\setlength{\headsep}{3ex} % must be *before* \input{hogg_nasa}
\input{hogg_nasa}
\pagestyle{myheadings}
\markboth{}{\color{gray}\sffamily New tools for extreme-precision spectrographs}
\begin{document}

\noindent
The National priorities of detection, validation, and characterization of exoplanets all depend critically on extremely precise radial-velocity (RV) measurement, or EPRV.
Extreme-precision radial-velocity measurement is also critical to current NASA missions:
NASA is a partner in the \textsl{NEID} spectrograph, NASA \textsl{TESS} discoveries are validated and characterized with RV measurements, spectroscopic targets for NASA \textsl{JWST} are identified and vetted with RV surveys, and the feasibility and design of future space-based IR--UV facilities depend on both individual RV-characterized planets and also the statistics of planet populations.

\section*{Why is close-to-the-metal software critical to EPRV?}

This proposal consists of three activities, or work packages, WP1, WP2, and WP3, in which we create methods, build open-source software, and write papers for the refereed literature, all of which support the making of extremely precise radial-velocity measurements with EPRV spectrographs.
Importantly, these contributions are very close to the metal, in the sense that they support or replace parts of the pipelines that go from raw spectrograph readouts to RV measurements.
Our view is that attention to detail is warranted at every level in these projects, in part because small amounts of information can be lost at every stage, and this is a game of centimeters per second, or ten-thousandths of a pixel in contemporary instruments, and in part because these problems lead to many intellectually rich and important problems in data analysis and software.

The error budget for precision RV measurement is complex, including terms from hardware stability, calibration noise, data processing, atmospheric absorption, spectral templates, and stellar variability and activity \cite[eg,][]{Halverson}. This proposal focuses on ideas, algorithms, and software that support the 1D spectral data extraction and RV measurements themselves, working close to the metal. This level in the problem is important for many reasons, but the most important two are: (A) The parts of the problems that are close to the metal are usually relegated to instrument teams and often relies on legacy code; it is time for new open-source, extensible, and sustainably maintained platforms and approaches. (B) Spectral extraction and RV measurement are both absolutely critical to the end-to-end scientific program; if any resolution, signal-to-noise, or information is sacrificed at any point in this process, the community is not using its hardware capabilities to their fullest potential.

\hoggbox{\paragraph{Box 1: Information Theory:}
The fundamental limit on \EPRV\ precision is given by the Cram\'er--Rao Bound, or
the Fisher information.
The Fisher information is the best possible inverse-variance that can be obtained
on any \RV\ measurement, and it appears in the literature,
although not usually written in this language \cite{Butler1996, Bouchy2003}.
Symbolically, in the time-invariant case,
the information $\sigma_v^{-2}$ is given by
\begin{equation}\label{eq:fisher}
\sigma_v^{-2} =
\left[\frac{\dd f}{\dd v}\right]^{\mathsf T}\cdot C^{-1}\cdot
\left[\frac{\dd f}{\dd v}\right] \quad ,
\end{equation}
where $\sigma_v$ is the \RV\ uncertainty, the derivatives are
of the spectral expectation $f$ (seen as a column vector) with respect to \RV\ $v$,
and the inverse variance of the noise in the spectrum is $C^{-1}$ (diagonal in the
simple case that all spectral pixels are independently measured).

Things get a bit more complicated when there are nuisance parameters, such as
the detailed telluric and spectral templates and their variations with time.
But nuisance parameters do not change this fundamentally: The derivatives
become rectangular projection operators and the information becomes a matrix
that must be inverted to deliver nuisance-marginalized uncertainties.
}

To expand on the above, it is critical to innovation, sustainabilty, and propagation of best practices in close-to-the-metal infrastructure that we build and support not just software in this area, but communities of people working in this area.
There is a lively intellectual community working in these areas, but they tend to work in their individual instrument silos, and good ideas don't always propagate to new projects.
For this reason, an important part of this proposal is to build and support the community of instrument pipeline and software developers.
In addition to building good documentation, including use cases and tutorials, and in addition to keeping the code issues list open and editable by the world, and in addition to taking pull requests and interacting with the open-source community, this proposal also includes two community workshops, to be held at a great meeting venue [details redacted].
These meetings will serve multiple goals.
One is to support and help our user base.
Another is to support and encourage potential future developers for our products.
But the most important is to bring together and nucleate inter-instrument and inter-group collaborations and conversations about pipeline details that (because they are close to the metal) often get ignored in discussions of exoplanet science.

In what follows, we will make frequent references to information theory and information-theoretic bounds.
We provide a quick review in Box~1.

\section{WP1: A flexible, extensible, data-driven method for measuring RVs}

This proposal supports three activities, or work packages. The first (WP1) is to build an open-source, data-driven model for one-dimensional high-resolution stellar spectroscopy that has good causal structure and is highly extensible, based on the success of the pioneering software and method known as \textsl{wobble} \cite{Bedell2019}.
This new model will be both algorithms and code, the latter in community-supported, open-source python and jax \cite{jax}.
The model can be used to make RV measurements that demonstrably saturate information-theoretic bounds, to build from multi-epoch observations empirical stellar and telluric templates, and to learn (and learn from) stellar and telluric variability.
Importantly, it will be part of WP1 to extend the data-driven model to include realistic (but also data-driven) models of stellar spectral variability.
The code built in WP1 will be designed with extensibility, maintainability, and community engagement as top design considerations.

Briefly, data-driven approaches to RV measurement work as follows. This is a cartoon, but it is roughly accurate:
\begin{itemize}
    \item \emph{Initialization:} Get a good first guess at all of the RVs for all epoch spectra on an individual target star, using (perhaps) the instrument standard pipeline outputs.
    \item Doppler-shift the extracted and continuum-normalized epoch spectra to a common rest-frame wavelength grid, using the current beliefs about the RVs, and average the spectra to build an empirical spectral template.
    \item Re-measure the individual epoch RVs by cross-correlation (or likelihood optimization) using that average spectrum as a template.
    \item Iterate Doppler-shifting and averaging, followed by cross-correlation, to convergence.
\end{itemize}
That's not precisely how the \textsl{wobble} code works, and it isn't precisely how the code we develop here will operate!
This is especially true because we will also include a stellar-variability model, and a tellurics model.
However, this cartoon algorithm gives a sense of why the data-driven method is a good idea.
And although there aren't currently proofs, convergence is generally fast \cite{Bedell2019}.
The above ``algorithm'' is a cartoon, because in fact the approach we will implement here involves writing a likelihood function for the (time-variable) telluric components, the (time-variable) stellar spectral template, and all the RVs.

The RV measurements made by most (all?) contemporary pipelines are made according to the following steps. Again, this is a cartoon:
\begin{itemize}
    \item Cross-correlate the individual extracted and continuum-normalized 1D spectrum with a binary mask, where that binary mask puts weight on or near expected rest-frame spectral features for the star. Compute this cross-correlation function (CCF) on a fine grid in stellar RV.
    \item Measure the peak of the CCF by fitting the CCF with a velocity-spacea line-spread function (LSF). This LSF is is often modeled as a Gaussian or a mixture of Gaussians.
    \item Report the center or peak of this LSF fit as the RV measurement.
\end{itemize}
This method is good for pipelines because it is simple, it can be used to mask bad regions (it's literally a binary mask), and it acts on every spectral observation independently.

In forthcoming work ([names redacted], forthcoming), we analyze this procedure and develop a set of comments.
One is that this traditional procedure (make CCF with a binary mask, fit CCF with a LSF) \emph{can} saturate the information-theoretic bounds on RV precision, under deep assumptions.
Some of the deep assumptions are about the calibration and noise properties of the extracted 1D spectral pixels.
But a critical, deep assumption is that the binary mask and the LSF be ``correct'' in a very specific sense.
The sense is that the LSF-convolved binary mask (or its negative, maybe, depending on sign conventions) should look very much like the expected value of the continuum-normalized 1D spectrum.
That is, the LSF-convolved mask is required to be a good model for the data (in the mean).

Data-driven methods improve on this in several ways, but the very most important is that they can develop spectral templates that are extremely accurate in the mean.
That is, because the template can be (something like) the average of the epoch spectra, the template is, by construction, a good model for the mean stellar spectral expectation.
Of course there is a detail that the RV measurements must be good enough to align the spectra well for averaging!
But currently all methods perform well enough to get this alignment-and-averaging correct.
The differences between current RV measurements and the information-theoretic bounds are measured in thousandths of a pixel (way, way smaller than the spectrograph resolution element).

Preliminary experiments suggest that once a star has a dozen epoch spectra, the empirical average spectrum used as a template out-performs most (LSF-convolved) binary masks in terms of being a good model for the stellar data.
That is, in standard observing campaigns on stars, data-driven stellar RV measurements with an empirical template should out-perform LSF-convolved binary-mask cross-correlations.
This becomes more and more true as stars get cooler, because cool stars do not have good binary-mask templates, and the lines strongly overlap in large parts of the spectrum (such that the whole binary-mask approach is called into question).

As we have implied, the method, code, documentation, and literature we will write for WP1 owes a lot to \textsl{wobble} \cite{Bedell2019}.
Briefly, the \textsl{wobble} approach is to optimize a regularized cost function (which can be thought of as a likelihood function or a posterior) with the following properties:
\begin{align}
    HOGG &: FOO \\
    \delta x & = \ln{\sqrt{\frac{1 + \beta}{1 - \beta}}} \\
    \hat{y}_{i,j} & = f (\boldsymbol{x}_i + \Delta x_{j}; \boldsymbol{\theta}_n ) \\
    \mathcal{L} & = \sum_{i,j} \frac{(y_{i,j} - \hat{y}_{i,j})^2}{\sigma_{i,j}^2} \\
    \hat{\boldsymbol{\theta}}_n & = min (\mathcal{L}(\boldsymbol{\theta}_n)) ~,
\end{align}
where...

Our dear reviewer might ask, since \textsl{wobble} exists and functions, why write something new? Why compete with this?
There are a few reasons:
\begin{itemize}
    \item The \textsl{wobble} method is an extremely good idea, and our only information-saturating hope for cool stars at the present day; it deserves our attention!
    \item Fundamentally, the published version of \textsl{wobble} is primarily an idea or a method, not (primarily) a set of code. Although there is code associated with that project, the code is prototype code intended for limited uses, and requiring a significant amount of hand-holding when brought to new contexts.
    \item Since the current \textsl{wobble} was released, there has been great innovation in data-analysis and machine-learning infrastructure in the Python ecosystem, most especially Python \textsl{jax}, which is a framework for building very sophisticated yet easy-to-support models and coupling them to optimization.
    \item There are new ideas about wavelength calibration, and simultaneous reference, that can be incorporated into the model if it is properly extensible.
    \item There are several new spectrographs coming on-line, and especially NASA \textsl{NEID}, which will be looking to have best-in-class data-analysis pipelines or innovate in the pipeline space.
\end{itemize}
It is not traditional for individual-investigator grants to support re-writes and new versions of existing ideas and methods.
However, exoplanet research is so vital to NASA's mission, and spectral data-analysis code is so vital to exoplanet research (both detection and characterization), we believe that WP1 is a very high science-per-dollar application for the XRP program.
It will support many missions and countless science goals.

... Note that we get improve tellurics for free! How do current pipelines deal with tellurics? Also micro-tellurics: If they can affect RV, they will be visible to wobble.

... Regularization

There are many other points of possible intervention in the EPRV pipelines that might be relevant to maximizing RV precision from a finite set of data.
For example, the 1D extraction methods currently in use sacrifice some precision (see WP3 for discussion of this; this proposal is also to work on extraction).
For another example, instrument wavelength calibration and flat calibration can be improved by building hierarchical models \cite{excalibur}.
For another, noise models in the spectrograph are currently na\"ive, and there are improvements that could be made with only limited additional computational and code complexity.
For yet another, the objective of continuum normalization is somewhat ill-posed, and continuum normalization methods currently in play might be inappropriate for cooler stars.
All of these points are worthy of research and engineering.
This proposal is not committing to anything beyond the work packages WP1, WP2, and WP3, but the group of the PI is looking at all of these considerations as well.

What are our deliverables?

\section{WP2: A realistic, instrument-agnostic simulator for EPRV data.}
% The second activity (WP2) is to build an open-source spectroscopic data simulator. This simulator will combine highest-available-resolution models of stellar photospheres and the atmosphere with effective models of spectrograph line-spread functions, pixelizations, calibrations, and noise models to produce highly realistic simulated RV data. The simulator can produce realistic 2D or 1D data, and thus test not just 1D data analysis methods but also 2D extraction methods. The simulator will be flexible such that it can simulate data for any spectrograph; as part of WP2 it will be delivered with settings pre-determined for a few high-impact spectrographs including NASA NEID.

\subsection{Method}
The simulator produced in WP2 will not be used to create toy data, but attempt to make data as close as possible to real spectrograph results by combining highest resolution theoretical models for emission and transmission and carefully processing these models to mimic a detector. A diagram of this process is presented below. To start the simulation, we need a model for the spectral emission of star. We choose the PHOENIX models, which were generated using 3D models of the photosphere of stars of varying temperature, surface gravity, metalicity, and helium content. This package uses \text{astropy} units to ensure accuracy of units transformations. The user then has to option of adding one or many transmission models to the simulation. In the diagram, we show that if the user wishes to simulate the KECK HIRes spectrograph, they can add transmission models for both the atmosphere and the internal gas cell. 

Since all these models for flux and transmission are not likely created on the same wavelength grids, they must be interpolated onto a common grid before they can be combined. In order to preserve the information in each model, the simulator creates a new linearly-spaced grid in log wavelength with $\Delta x = min(median(\Delta x_i))$, where $\Delta x_i$ refers to the spacing in log wavelength of model i. Then, each model is interpolated unto this new grid using cubic splines in flux, and log wavelength. The emission flux grid of the star is the multiplied by each of the transmission grids to get a total flux.


\tikzset{%
  block/.style    = {draw, thick, rectangle, minimum height = 3em,
    minimum width = 3em},
  circ/.style      = {draw, circle, node distance = 2cm}, % Adder
  input/.style    = {coordinate}, % Input
  output/.style   = {coordinate} % Output
}
    
\pagebreak

\begin{figure}[p!]

        \begin{tikzpicture}[auto, thick, node distance=2cm, >=triangle 45]
        \draw
        	% Drawing the blocks of first filter :
        	node at (1,0) [block, name=star, fill=red!30] {star} 
        	node [block, below of=star, fill=blue!30] (tell) {tellurics}
        	node [block, below of=tell, fill=green!30] (gas) {gas}
    	    node [circ, right of=star, fill=red!30] (starw) {$x^{*}_{s}$}
            node [circ, right of=tell, fill=blue!30] (tellw) {$x^{*}_{t}$}
            node [circ, right of=gas, fill=green!30] (gasw) {$x^{*}_{g}$}
        	
            node [circ, right of=starw, fill=red!30] (starf) {$f^{*}_{s}$}
            node [circ, right of=tellw, fill=blue!30] (tellf) {$f^{*}_{t}$}
            node [circ, right of=gasw, fill=green!30] (gasf) {$f^{*}_{g}$}
            
            node [block, below of=gasw] (newgrid) {$min(med(\Delta x_i))$}
            node at (6,-6) [circ] (xthe) {$x^{the}$}
            
            node [block, right of=starf] (splines) {$C$}
            node [block, right of=tellf] (splinet) {$C$}
            node [block, right of=gasf] (splineg) {$C$}
            
            node [circ, right of=splines, fill=red!30] (fsthe) {$f^{the}_s$}
            node [circ, right of=splinet, fill=blue!30] (ftthe) {$f^{the}_t$}
            node [circ, right of=splineg, fill=green!30] (fgthe) {$f^{the}_g$}
            
            node [circ, right of=fsthe] (mult1) {$\times$}
            node [block, below of=fgthe] (res) {$R(\lambda)$}
            node [block, right of=res] (lsf) {LSF}
            
            node [circ, right of=fgthe, fill=gray!30] (ftot) {$f_{tot}$}
            
            node [circ, right of=ftot] (conv) {$\otimes$}
            node [circ, below of=conv, fill=pink!30] (flsf) {$f_{lsf}$}
        ;
        
        \draw
            node at (1,-10) [circ,name=xdet] {$x_{det}$}
            node [block, right of=xdet] (wavet) {WT}
            node [circ, right of=wavet] (xhat) {$\hat{x}$}
            node [block, right of=xhat] (lanc) {$L$}
            node [circ, right of=lanc] (fexp) {$f_{exp}$}
            
            node [circ, right of=fexp] (nexp) {$N_{exp}$}
            
            node [block, right of=nexp] (snrt) {$SNR_{true}$}
            node [circ, below of=snrt] (noise) {$n$}
            node [circ, below of=nexp] (add) {$+$}
            node [circ, left of=add] (fread) {$N_{read}$}
            node [block, left of=fread] (snrr) {$SNR_{read}$}
            node [circ, left of=snrr] (ferr) {$\sigma_N$}
        ;
            \draw[->](xdet) -- node {}(wavet);
            \draw[->](wavet) -- node {} (xhat);
            \draw[->](xhat) -- node {} (lanc);
            \draw[->](lanc) -- node {} (fexp);
            \draw[->](fexp) -- node {} (nexp);
            \draw[->](nexp) -- node {} (snrt);
            \draw[->](nexp) -- node {} (add);
            \draw[->](snrt) -- node {} (noise);
            \draw[->](noise) -- node {} (add);
            \draw[->](add) -- node {} (fread);
            \draw[->](fread) -- node {} (snrr);
            \draw[->](snrr) -- node {} (ferr);
            
            \draw[->](xthe) to[out=-90,in=100] node {} (lanc);
            \draw[->](flsf) to[out=-90,in=80] node {} (lanc);
            % Joining blocks. 
            % Commands \draw with options like [->] must be written individually
        	\draw[->](star) to[out=30,in=150] node {}(starf);
         	\draw[->](tell) to[out=30,in=150] node {} (tellf);
        	\draw[->](gas) to[out=30,in=150] node {} (gasf);
        	\draw[->](star) -- node {}(starw);
         	\draw[->](tell) -- node {} (tellw);
        	\draw[->](gas) -- node {} (gasw);
            \begin{scope}[on background layer]
                \draw[dotted](starw) -- node {} (newgrid);
             	\draw[dotted](tellw) -- node {} (newgrid);
            	\draw[dotted](gasw) -- node {} (newgrid);
            \end{scope}
            \draw[->](newgrid) -- node {}(xthe);
            
            \draw[->](starf) -- node {} (splines);
         	\draw[->](tellf) -- node {} (splinet);
        	\draw[->](gasf) -- node {} (splineg);
        	
        	\draw[->](starw) to[out=-45,in=-135] node {$+\Delta_i$} (splines);
         	\draw[->](tellw) to[out=-30,in=-150] node {} (splinet);
        	\draw[->](gasw) to[out=-30,in=-150] node {} (splineg);
        	
        	\draw[dotted](xthe) to[out=30,in=-90] node {} (splineg);
        	\draw[dotted](splineg) -- node {} (splinet);
        	\draw[dotted](splinet) -- node {} (splines);
        	
    		\draw[->](splines) -- node {} (fsthe);
         	\draw[->](splinet) -- node {} (ftthe);
        	\draw[->](splineg) -- node {} (fgthe);
        	
        	\draw[->](fsthe) -- node {} (mult1);
         	\draw[->](ftthe) -- node {} (mult1);
        	\draw[->](fgthe) -- node {} (mult1);
        	
        	\draw[->](mult1) -- node {} (ftot);
        	\draw[->](xthe) -- node {} (res);
        	\draw[->](res) -- node {} (lsf);
        	\draw[->](lsf) -- node {} (conv);
        	\draw[->](ftot) -- node {} (conv);
        	\draw[->](conv) -- node {} (flsf);
        	
        	% Adder
        	% Boxing and labelling noise shapers
        	\draw [color=gray,thick](-0.5,-7) rectangle (14,1);
        	\node at (-0.5,1) [above=5mm, right=0mm] {\textsc{Theory}};
        	\draw [color=gray,thick](-0.5,-13) rectangle (14,-9);
        	\node at (-0.5,-9) [above=5mm, right=0mm] {\textsc{Sampling}};
        \end{tikzpicture}
        \caption{Overview of the WP2}
    \end{figure}

\section{WP3: Information-preserving spectroscopic extraction}

Generally, EPRV proceeds by observations with a two-dimensional detector in a spectrograph focal plane, followed by extraction of a one-dimensional spectrum, or one one-dimensional spectrum per Echelle order.
Fundamentally, the single most important property of this extraction step is that it preserve the radial-velocity information.
That is, it is critical that the one-dimensional extracted spectrum contain all of the information about the stellar velocity that was present in the two-dimensional data.
We believe that there is currently no method for this extraction that meets this information-preservation requirement.
In activity WP3 our goal is to \textbf{produce a method and code for one-dimensional spectral extraction that preserves as much of the radial-velocity information as possible}.

For these purposes, ``information'' will be defined as in Box~1: It is the expected inverse variance in a measurement or estimate made with a (justified) maximum-likelihood estimator.
If one observation leads to a higher expected inverse variance than another observation, then the one has higher information content or more information.
Inverse variance is the Fisher information, but it is also the property of the observation that increases linearly with time, when observations are either photon-limited or background-limited.

Of course, technically, in an end-to-end engineering system, there are trades and losses in every component.
So some loss of radial-velocity information at the extraction step is permissible, provided that the pipelines meet requirements overall.
However, if there are low-cost or no-cost improvements to pipelines that lead to the preservation and transmission of more radial-velocity information, then these improvements are extremely high value in science-per-dollar (say) terms.
One way to see this is that a pipeline improvement that leads to (say) a 10-percent improvement in information in the extracted spectrum is equivalent to a telescope or hardware improvement that leads to a 10-percent reduction in exposure times, or a grant that delivers a 10-percent increase in observing time.
It will deliver a squared signal-to-noise that is better by 10-percent on any planet discoveries or characterizations.

It is therefore both an ethical requirement and an engineering requirement that NASA-funded projects be saturating their information bounds, when such saturation can come at very low cost.
In WP3 we will produce a method for information-preserving spectral extraction, a publication describing this in the refereed literature, and open-source code with a reference implementation.
This method and code will be available to be integrated into the NEID pipeline, and the pipelines of any other EPRV spectrographs, globally.

At the present day, there are many different methods and codes for spectral extraction.
In the EPRV community, many of the instrument pipelines use some form of flat-relative optimal extraction (FROE) \cite{froe}.
Briefly, FROE works by comparing the 2D spectral image to the 2D image of a flat-field calibration exposure (usually a spectrum of a featureless blackbody calibration lamp).
Define ``columns'' and ``rows'' such that the 2D image has columns that cut across the spectral trace (or the trace of an echelle order) and rows that go along (or nearly along) the spectral trace.
In FROE, the idea is, at each order, the 2D image of the calibration lamp spectrum is pistoned up and down, with a different piston value at every column, until the pistoned calibration image looks as close as possible to the science exposure.
The extracted one-dimensional spectrum is then the values of these individual-column piston values.
Technically FROE returns the extracted one-dimensional \emph{ratio spectrum} between the science exposure and the flat-field exposure.

The FROE concept is beautiful, simple, and effective, because it uses the calibration data directly to build a forward model of the science data.
That is, the pistoned flat-field 2D image is a generative model for the 2D science image.
That's what makes the method ``optimal'': It is a forward model for the raw data, and it can be compared to the raw data with a justified likelihood function or loss function with a proper noise model.
It is also very simple to implement; it permits a generative model of the raw data without the requirement of a full instrument model for the spectrograph optical image.

However, in its current form, FROE sacrifices a small amount of radial-velocity information when applied to most contemporary spectrographs.
The reason is that the directions on the spectrograph focal plane that are normal to the wavelength direction (or the iso-wavelength surfaces) do not align perfectly with the detector pixel grid.
Indeed, the requirement that these align is an important part of the design of the \textsl{EXPRES} spectrograph \cite{expres}, and led to significant hardware design complexity and cost.
For most spectrographs, this alignment is not perfect, and even when alignment is a design goal, it can't be satisfied to arbitrary accuracy.
Thus there are always alignment issues, between wavelength and the detector grid, and FROE (in its standard form) will sacrifice some spectral resolution (and hence RV precision).

One alternative to FROE, which does account for these spectral-direction alignment issues, is SpectroPerfectionism (SP) \cite{sp}.
In this method, the 2D science image is modeled using a complete instrument model, which knows the full ``point-spread function'' (PSF) or probability distribution, given a photon wavelength, for the 2D pixel location at which that photon will be detected.
Like FROE, this model is generative---it extracts by making a forward model of the 2D data---but it is different in that it requires (and benefits from) a full instrument model.
Despite this, SP also does not completely preserve the RV information in its current standard form.
The reason is that by using the full PSF in its model, SP builds a model for how an \emph{infinite resolution} 1D spectral input would illuminate the focal plane.
That leads to ringing, which in turn is damped out with a post-model reconvolution of the results.
This reconvolution step is reminiscent of a part of the CLEAN algorithm in radio astronomy \cite{clean}.
But this reconvolution step---which is technically unavoidable given the model structure---removes some RV information.

Technically, SP could be made without the reconvolution step.
However, the reconvolution is necessary to reduce degeneracies and near-degeneracies when models are built at extremely high resolution.
In principle SP might be improved from an EPRV standpoint with a spectral-model regularization that controled these degeneracies.
However, in WP3 we will build our method as an extension of FROE rather than SP.
One reason for working on FROE is that the method \emph{is currently the method of choice} in the EPRV community.
Another is that it requires very little knowledge about the instrument, far less than is required by SP.

What is our extension to FROE?

What are our deliverables?

\section{Other projects and stretch goals}

Simulating 2-d image.

Mashing up spectral variability models.

Combining telluric information from many different stars.

Doing RV in the 2-d images.

\section{Community workshops}

For both WP1 and WP2, code packages will be produced along with documentation on how to appropriately use and add modules to these programs. However, to build an client base that is invested in furthering this project, we will also plan community workshops with the clients that are interested using the packages. These community workshops will be organized through the Flatiron Institute. Several similar workshops have been held there in the past for Tellurics Hack week, Preparing for TESS, and Gaia Sprints. The goal of these workshops will not only to be getting users familiar with the package, but also gain an insight into what the community would find most useful additions to each package. This will change the priorities of our timeline if the clients have preference on what is most important to the future of EPRV.

\section{Data management plan}

This project will not produce new data per se, but it does produce methods, code, and documents (code documentation and scientific papers). It might also produce model parameters or components of spectral models. All of the code, parameters, models, documentation, and all written documents relating to the project will be maintained in publicly visible version-control repositories on the GitHub and GitLab platforms. These repositories can be cloned and reproduced at no cost by any user in the world.

The code and documents will be licensed for re-use by others under permissive licenses (MIT for code, CC-by for documents). This permits re-use, reproducibility, and re-publication by others.

Although we do not expect to deliver large data sets, if we do create any catalog of stellar radial velocities or RV corrections or other per-observation data or meta-data, we will volunteer these to the HARPS and ESPRESSO Archives (as appropriate) for hosting and preservation. If the Archives are not interested, we will host these data on versioned and mirrored Github and GitLab repositories, both of which accept data sets as large as anything that we could produce here. Once again, these would be globally available and licensed for arbitrary re-use.

Any cite-able data, code, or documentation will be delivered to Zenodo for preservation and creation of a lasting and cite-able DOI. All documents and papers will be submitted to the arXiv and also AAS-family journals for long-term preservation, cite-ability, and publication.


\section{Timeline}

\section{Summary of deliverables}
The deliverables from WP1 will be open-source code, methods, and papers for data-driven modeling of EPRVs. This will also include workshops, and documentation with clear explanation of both how to use and contribute to further this project.

The deliverables from WP2 will also include open-source code, methods, and papers for modular simulator of spectrographs.

\section{Frequently asked questions}

\clearpage\raggedright %
\bibliographystyle{plain}
\bibliography{xrp}

\end{document}