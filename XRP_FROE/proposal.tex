% to-do
% -----
% - Put in Daunt references in the introduction.
% - Explicitly reference the Gaps document from JPL.
% - Go through and boldface important points, about two per page.

\documentclass[12pt]{article}
\setlength{\headheight}{2ex} % must be *before* \input{hogg_nasa}
\usepackage{amsmath}
% \usepackage{physics}
\setlength{\headsep}{3ex} % must be *before* \input{hogg_nasa}
\input{hogg_nasa}
\pagestyle{myheadings}
\markboth{}{\color{gray}\sffamily New tools for extreme-precision spectrographs}

\tikzset{%
  block/.style    = {draw, thick, rectangle, minimum height = 3em,
    minimum width = 3em},
  circ/.style      = {draw, circle, node distance = 2cm}, % Adder
  input/.style    = {coordinate}, % Input
  output/.style   = {coordinate} % Output
}

\begin{document}\setcounter{page}{0}

\noindent{\sffamily%
The National priorities of detection, validation, and characterization of exoplanets all depend critically on extremely precise radial-velocity (RV) measurement. It is also critical to current NASA missions: NASA is a partner in the \project{NEID} spectrograph, \project{TESS} discoveries are validated and characterized with RV measurements, spectroscopic targets for \project{JWST} are identified and vetted with RV surveys, and the feasibility and design of future space-based IR-UV facilities depend on both individual RV-characterized planets and also the statistics of planet populations.

The error budget for precision RV measurement is complex, including terms from hardware stability, calibration noise, data processing, atmospheric absorption, spectral templates, and stellar variability and activity. This proposal focuses on ideas, algorithms, and software that support the 1D spectral data extraction and RV measurements themselves, working close to the metal. This is important: (A) These parts of the problem are usually relegated to instrument teams and often relies on legacy code; it is time for new open-source, extensible, community-driven, and sustainably maintained platforms. (B) Spectral extraction and RV measurement are both absolutely critical to the end-to-end scientific program; if any resolution, signal-to-noise, or information is sacrificed at any point in this process, the community is not using its hardware capabilities to their fullest potential.

The proposal supports three activities. The first (WP1) is to build an open-source, data-driven model for one-dimensional high-resolution stellar spectroscopy that has good causal structure and is highly extensible. This model will be both algorithms and code, the latter in community-supported python and jax. The model can be used to make RV measurements that demonstrably saturate information-theoretic bounds, to build from multi-epoch observations empirical stellar and telluric templates, and to learn (and learn from) stellar and telluric variability. The code built in WP1 is designed with extensibility, maintainability, and community engagement as top design considerations.
The second activity (WP2) is to build an open-source spectroscopic data simulator. This simulator will combine highest-available-resolution models of stellar photospheres and the atmosphere with effective models of spectrograph line-spread functions, pixelizations, calibrations, and noise models to produce highly realistic simulated RV data. The simulator can produce realistic 2D or 1D data, and thus test not just 1D data analysis methods but also 2D extraction methods. The simulator will be flexible such that it can simulate data for any spectrograph; as part of WP2 it will be delivered with settings pre-determined for a few high-impact spectrographs including NASA NEID.
The third activity (WP3) is to analyze and extend 2D extraction algorithms with an eye to maximum RV information transmission to the end user. The flat-relative optimal extraction (FROE) and spectro-perfectionism (SP) models for extraction of 1D spectra from 2D spectrograph images both make assumptions that slightly degrade spectral resolution and sacrifice Fisher information. In both cases, these assumptions can be weakened. As part of WP3, new algorithms and open-source code will be produced. In the long run, RV measurements may be made directly in the 2D imaging, without any explicit spectral extraction. Analysis of this possibility is also part of WP3.

In addition to delivering algorithms, open-source code, documentation, and refereed scientific papers for each of the three activities (WP1, WP2, WP3), the proposal includes community workshops to build and support the National and Global hardware and software communities working on RV measurements.}

\section*{Why is close-to-the-metal software critical to EPRV?}

This proposal consists of three activities, or work packages, WP1, WP2, and WP3, in which we create methods, build open-source software, and write papers for the refereed literature, all of which support the making of extremely precise radial-velocity measurements with EPRV spectrographs.
Importantly, these contributions are very close to the metal, in the sense that they support or replace parts of the pipelines that go from raw spectrograph readouts to RV measurements.
Our view is that \textbf{close attention to detail is warranted at every level in these projects, in part because small amounts of information can be lost at every stage}, and this is a game of centimeters per second, or ten-thousandths of a pixel in contemporary instruments, and in part because these problems lead to many intellectually rich and important problems in data analysis and software.

The error budget for precision RV measurement is complex, including terms from hardware stability, calibration noise, data processing, atmospheric absorption, spectral templates, and stellar variability and activity \cite[eg,][]{Halverson}. This proposal focuses on ideas, algorithms, and software that support the 1D spectral data extraction and RV measurements themselves, working close to the metal. This level in the problem is important for many reasons, but the most important two are: (A) The parts of the problems that are close to the metal are usually relegated to instrument teams and often rely on legacy code; it is time for new open-source, extensible, and sustainably maintained platforms and approaches. (B) Spectral extraction and RV measurement are both absolutely critical to the end-to-end scientific program; if any resolution, signal-to-noise, or information is sacrificed at any point in this process, the community is not using its hardware capabilities to their fullest potential.

To expand on the above, it is critical to innovation, sustainabilty, and propagation of best practices in close-to-the-metal infrastructure that we build and support not just software in this area, but communities of people working in this area.
There is a lively intellectual community working in these areas, but they tend to work in their individual instrument silos, and good ideas don't always propagate to new projects.
For this reason, an important part of this proposal is to build and support the community of instrument pipeline and software developers.
In addition to building good documentation, including use cases and tutorials, and in addition to keeping the code issues list open and editable by the world, and in addition to taking pull requests and interacting with the open-source community, this proposal also includes two community workshops, to be held at a great meeting venue.
These meetings will serve multiple goals.
One is to support and help our user base.
Another is to support and encourage potential future developers for our products.
But the most important is to bring together and nucleate inter-instrument and inter-group collaborations and conversations about pipeline details that (because they are close to the metal) often get ignored in discussions of exoplanet science.

It seems strange to justify the importance of this kind of work explicitly in the context of the NASA \project{XRP} call!
After all, EPRV is core to everything we do in exoplanet research.
EPRV led to the first planet discoveries \cite{mayor}; EPRV has been responsible for many hundreds of more since; and EPRV is the foremost method for confirming and characterizing transit discoveries.
It will remain core---or maybe become even more important---in the coming era of astrometric planet discovery.
All of these activities are central to NASA's short and long-term priorities, and the priorities of all of US astrophysics. Even to all of global astrophysics!
Since EPRV is on the critical path for a large part of exoplanet science, the close-to-the-metal part of EPRV is important, technically and intellectually, to an extremely wide range of projects.
Indeed, \textbf{improvements at the raw-data level here will pay dividends in hundreds of individual exoplanet projects} going on now and far more in the future. This proposal is to build and strengthen this part of EPRV science.

\hoggbox{\paragraph{Box 1: Information Theory:}
The fundamental limit on \EPRV\ precision is given by the Cram\'er--Rao Bound, or
the Fisher information.
The Fisher information is the best possible inverse-variance that can be obtained
on any \RV\ measurement, and it appears in the literature,
although not usually written in this language \cite{Butler1996, Bouchy2003}.
Symbolically, in the time-invariant case,
the information $\sigma_v^{-2}$ is given by
\begin{equation}\label{eq:fisher}
\sigma_v^{-2} =
\left[\frac{\dd f}{\dd v}\right]^{\mathsf T}\cdot C^{-1}\cdot
\left[\frac{\dd f}{\dd v}\right] \quad ,
\end{equation}
where $\sigma_v$ is the \RV\ uncertainty, the derivatives are
of the spectral expectation $f$ (seen as a column vector) with respect to \RV\ $v$,
and the inverse variance of the noise in the spectrum is $C^{-1}$ (diagonal in the
simple case that all spectral pixels are independently measured).

Things get a bit more complicated when there are nuisance parameters, such as
the detailed telluric and spectral templates and their variations with time.
But nuisance parameters do not change this fundamentally: The derivatives
become rectangular projection operators and the information becomes a matrix
that must be inverted to deliver nuisance-marginalized uncertainties.
}

In what follows, we will make references to information theory and information-theoretic bounds.
We provide a quick review in Box~1.

\section{WP1: An extensible, data-driven method for measuring RVs}

This proposal supports three activities, or work packages. The first (WP1) is to build an open-source, data-driven model for one-dimensional high-resolution stellar spectroscopy that has good causal structure and is highly extensible, based on the success of the pioneering software and method known as \textsl{wobble} \cite{Bedell2019}.
This new model will be both algorithms and code, the latter in community-supported, open-source \project{Python} and \project{jax} \cite{jax}.
The model can be used to make RV measurements that demonstrably saturate information-theoretic bounds, to build from multi-epoch observations empirical stellar and telluric templates, and to learn (and learn from) stellar and telluric variability.
Importantly, it will be part of WP1 to extend the data-driven model to include realistic (but also data-driven) models of stellar spectral variability.
The code built in WP1 will be designed with extensibility, maintainability, and community engagement as top design considerations.

Briefly, data-driven approaches to RV measurement work as follows. This is a cartoon, but it is roughly accurate:
\begin{itemize}
    \item \emph{Initialization:} Get a good first guess at all of the RVs for all epoch spectra on an individual target star, using (perhaps) the instrument standard pipeline outputs.
    \item Doppler-shift the extracted and continuum-normalized epoch spectra to a common rest-frame wavelength grid, using the current beliefs about the RVs, and average the spectra to build an empirical spectral template.
    \item Re-measure the individual epoch RVs by cross-correlation (or likelihood optimization) using that average spectrum as a template.
    \item Iterate Doppler-shifting and averaging, followed by cross-correlation, to convergence.
\end{itemize}
That's not precisely how the code we propose here will operate! (More below.)
This is especially true because we will also include a stellar-variability model, and a tellurics model.
However, this cartoon algorithm gives a sense of why the data-driven method is a good idea.
And although there aren't currently proofs, convergence is generally fast \cite{Bedell2019}.
The above ``algorithm'' is a cartoon, because in fact the approach we will implement here involves writing a likelihood function for the (time-variable) telluric components, the (time-variable) stellar spectral template, and all the RVs.

The RV measurements made by most (all?) contemporary pipelines \cite[eg,]{harps} are made according to the following steps. Again, this is a cartoon:
\begin{itemize}
    \item Cross-correlate the individual extracted and continuum-normalized 1D spectrum with a binary mask, where that binary mask puts weight on or near expected rest-frame spectral features for the star. Compute this cross-correlation function (CCF) on a fine grid in stellar RV.
    \item Measure the peak of the CCF by fitting the CCF with a velocity-space line-spread function (LSF). This LSF is is often modeled as a Gaussian or a mixture of Gaussians.
    \item Report the center or peak of this LSF fit as the RV measurement.
\end{itemize}
This method is good for pipelines because it is simple, it can be used to mask bad regions (it's literally a binary mask), and it acts on every spectral observation independently.

In forthcoming work, we (the proposers) analyze this procedure and develop a set of comments.
One is that this traditional procedure (make CCF with a binary mask, fit CCF with a LSF) \emph{can} saturate the information-theoretic bounds on RV precision, under deep assumptions.
Some of the deep assumptions are about the calibration and noise properties of the extracted 1D spectral pixels.
But a critical, deep assumption is that the binary mask and the LSF be ``correct'' in a very specific sense.
The sense is that the LSF-convolved binary mask (or its negative, maybe, depending on sign conventions) should look very much like the expected value of the continuum-normalized 1D spectrum.
That is, the LSF-convolved mask is required to be a good model for the data (in the mean).

Data-driven methods improve on this in several ways, but the very most important is that they can develop spectral templates that are extremely accurate in the mean.
That is, because the template can be (something like) the average of the epoch spectra, \emph{the template is, by construction, a good model for the mean stellar spectral expectation}.
Of course there is a detail that the RV measurements must be good enough to align the spectra well for averaging!
But currently all methods perform well enough to get this alignment-and-averaging correct.
The differences between current RV measurements and the information-theoretic bounds are measured in thousandths of a pixel (way, way smaller than the spectrograph resolution element).

Preliminary experiments suggest that once a star has a dozen epoch spectra or more, the empirical average spectrum used as a template out-performs most (LSF-convolved) binary masks in terms of being a good model for the stellar data.
That is, in standard observing campaigns on stars, data-driven stellar RV measurements with an empirical template should out-perform LSF-convolved binary-mask cross-correlations.
This becomes more and more true as stars get cooler, because \textbf{cool stars do not have good binary-mask templates}, and the lines strongly overlap in large parts of the spectrum (such that the whole binary-mask approach is called into question).

As we have implied, the method, code, documentation, and literature we will write for WP1 owes a lot to the open-source method and code \textsl{wobble} \cite{Bedell2019}.
Briefly, our approach is to optimize a regularized cost function (which can be thought of as a negative-log-likelihood function $\mathcal{L}$) with the following properties:
\begin{align}
    \hat{\boldsymbol{y}}_n & = f (\boldsymbol{x} + \delta x_n; \boldsymbol{\theta}) + g(\boldsymbol{x}; \boldsymbol{\theta}) \\
    \mathcal{L}_n & = \frac{1}{2}\,\sum (\boldsymbol{y} - \hat{\boldsymbol{y}}_n)^\top\,\mathsf{C}^{-1}\,(\boldsymbol{y} - \hat{\boldsymbol{y}}_n) \\
    \hat{\boldsymbol{\theta}} &\leftarrow \arg\min_{\boldsymbol{\theta}}\sum_n\mathcal{L}_n \\
    \hat{\delta x}_n &\leftarrow \arg\min_{\delta x_n}\mathcal{L}_n
\end{align}
where $\boldsymbol{y}_n$ is logarithm of the data or the (vector) logarithmic spectrum from one epoch or observation $n$,
$\boldsymbol{x}$ is the (vector) of log-wavelengths for that spectrum (yes in the log),
$f()$ is the spectral expectation for the star,
$\delta x_n$ is the Doppler shift of the star at that epoch,
$\boldsymbol{\theta}$ is a huge set of spectral parameters,
$g()$ is the spectral expectation for the Doppler shift,
$\mathsf{C}$ is the effective inverse variance of the data in the log space,
and the spectral parameters are found by maximizing the log-likelihood (minimizing the negative log likelihood).
This simultaneously maximizes the likelihood for the parameters of the stellar spectrum and the tellurics absorption, and returns what is called a \emph{profile likelihood} for the individual-epoch Doppler shifts or RVs.

We can show that this method, used as a maximum-likelihood method, saturates the information-theoretic bounds for RV measurement in the case that the star is not varying; we will discuss varying the stellar spectrum some below.
This is even true when there is non-trivial tellurics contamination.
This method can also be used as an input to Bayesian or quasi-Bayesian approaches.
In detail this should be modified to marginalize rather than profile in the spectral parameters, but this difference is small when (as in EPRV contexts) the signal-to-noise is high.

One extremely important thing about this approach is that it explicitly learns a tellurics model along with the stellar spectral model.
Thus the spectrum is being modeled simultaneously with the tellurics, and any tellurics (even micro-tellurics) that are strong enough to affect the RV are strong enough to get modeled and corrected, naturally.
\emph{Most present-day pipelines mask only known tellurics}, which only accounts for strong telluric lines and isn't adaptive to the observatory or conditions (but see \cite{telluricPCA}).

Our dear reviewer might ask, since \textsl{wobble} exists and functions, why write something new?
There are a few reasons:
\begin{itemize}
    \item The \textsl{wobble} method is an extremely good idea, and our only information-saturating hope for cool stars at the present day; it deserves our attention!
    \item Fundamentally, the published version of \textsl{wobble} is primarily an idea or a method, not (primarily) a set of code. Although there is code associated with that project, the code is prototype code intended for limited uses, and requiring a significant amount of hand-holding when brought to new contexts.
    \item Since the current \textsl{wobble} was released, there has been great innovation in data-analysis and machine-learning infrastructure in the Python ecosystem, most especially Python \textsl{jax}, which is a framework for building very sophisticated yet easy-to-support models and coupling them to optimization.
    \item There are new ideas about wavelength calibration (such as dimensionality reduction \cite{excalibur}) and simultaneous reference that can be incorporated into the model if it is properly extensible.
    \item There are several new spectrographs coming on-line, and especially NASA \textsl{NEID}, which will be looking to have best-in-class data-analysis pipelines or innovate in the pipeline space.
\end{itemize}
It is not traditional for individual-investigator grants to support new versions of existing ideas and methods.
However, exoplanet research is so vital to NASA's mission, and spectral data-analysis code is so vital to exoplanet research (both detection and characterization), \textbf{WP1 is a very high science-per-dollar application for the XRP program}:
It will support many missions and countless science goals.

In addition to implementation of the basic model above, WP1 will involve investigations of, and implementations of various details and extensions:
\begin{description}
\item[Spectral representation]
Usually stellar spectra are represented as pixel values at pixel locations. This is effective, but it is a costly representation for a stellar spectrum. We are exploring Fourier representations, and representations as sums of stellar (or telluric) lines.
\item[Regularization]
The model has a huge amount of natural freedom. Regularization can improve optimization and remove subtle degeneracies. We will explore regularization methods and their tuning.
\item[Stellar variability]
The most important point here is that both the stellar spectrum and the telluric absorption vary with time. We will introduce low-dimensional models for such variability and fit for the low-dimensional variation. This is an important direction of research and we will only touch on it in WP1. One interesting thing is that the information-theoretic bounds on RV are unknown for time-variable stars; this is an issue that we will discuss---and develop a position on---as part of WP1. However, we will make the variability model in WP1 consistent with the kinds of effective models that are used for stellar variability at present, such as p-modes \cite{chaplin}, flares, spots on a rotating stellar surface \cite{soap, starry}, or purely data-driven approaches \cite{nnstellaractivity}.
\end{description}

There are many other points of possible intervention in the EPRV pipelines that might be relevant to maximizing RV precision from a finite set of data.
For example, the 1D extraction methods currently in use sacrifice some precision (see WP3 for discussion of this; this proposal is also to work on extraction).
For another example, instrument wavelength calibration and flat calibration can be improved by building hierarchical models \cite{excalibur}.
For another, noise models in the spectrograph are currently na\"ive, and there are improvements that could be made with only limited additional computational and code complexity.
For yet another, the objective of continuum normalization is somewhat ill-posed, and continuum normalization methods currently in play might be inappropriate for cooler stars.
All of these points are worthy of research and engineering.
This proposal is not committing to anything beyond the work packages WP1, WP2, and WP3, but the group of the PI is looking at all of these considerations as well.

The deliverables of WP1 will be open-source (MIT License) code, documentation, tutorials in which the code is run on standard, open data sets (such as data from the \project{HARPS} Archive or the \project{NEID} Archive), and a paper for the refereed literature.
We will also bring this method, code, and documentation to our first Community Workshop in Year 2, described below.

\section{WP2: A realistic, instrument-agnostic simulator for EPRV data.}
Simulating EPRV raw and extracted data is necessary for testing data-analysis pipelines.
It is also valuable for exploring sources of stellar and telluric variability, the effect of different kinds of noise sources in data, and instrument or extraction issues or artifacts.
In WP2 we propose to create an open-source simulator that can generate 1D or 2D simulated data, appropriate for any individual spectrograph (through a configuration interface or document).

At present, most pipelines and pipeline improvements are tested empirically with real data, with high-level comparisons made in terms of RV scatter or empirical RV precision.
Such comparisons don't flow down differences to assumptions or properties of the input data.
A simulator permits functional testing, and also experiments in which changes to the signal-to-noise, resolution, vignetting, noise model, and so on might be investigated for effects on pipeline outputs.

A critical motivation for WP2 is that it can be used to functionally test WP1.
But it will also be useful to any instrument team building a spectrograph or the associated pipelines.
In the Community Workshop in Years 2 and 3 of the project period (workshops are described below), we will bring together instrument team members and technically interested exoplanet scientists.
At these workshops, which will be in a primarily hands-on, hack-week style, we will build the configurations for the simulator to simulate data from any instruments that are represented, and build community support to maintain these configurations.
That is, \textbf{we are building a simulator for the whole EPRV community}, not just us.

Not just relevant for testing instruments and pipelines, a simulator is also useful for studying the effect of different kinds of stellar noise on end-to-end extraction and RV measurement, provided that those sources of noise can be modeled with some kind of effective model.
An example of this is the noise expected from star spots on a rotating stellar surface \cite{soap, starry}.
At present these are often modeled effectively as pure velocity shifts \cite{Aigrain}, and not in the spectral domain as spectral distortions that project onto velocity shifts by some overlap integral (or equivalent).

Going even further, the WP2 simulator can be used for instrument design.
The combination of WP1 and WP2 will permit science traceability from instrument properties, to best settings of algorithm hyperparameters, to resulting accuracy of extracted RVs. By simulating theoretical spectrographs with varying area, shot noise, room temperature, etc., we can return partial derivatives for how changes in the instrument parameters directly affect the precision the returned RVs. This science is valuable for planning future spectrographs and for setting bounds on instrument error.

\begin{figure}[t!]
\sffamily
        \begin{tikzpicture}[auto, thick, node distance=2cm, >=triangle 45]
        \draw
        	% Drawing the blocks of first filter :
        	node at (1,0) [block, name=star, fill=red!30] {star} 
        	node [block, below of=star, fill=blue!30] (tell) {tellurics}
        	node [block, below of=tell, fill=green!30] (gas) {gas}
    	    node [circ, right of=star, fill=red!30] (starw) {$x^{*}_{s}$}
            node [circ, right of=tell, fill=blue!30] (tellw) {$x^{*}_{t}$}
            node [circ, right of=gas, fill=green!30] (gasw) {$x^{*}_{g}$}
        	
            node [circ, right of=starw, fill=red!30] (starf) {$f^{*}_{s}$}
            node [circ, right of=tellw, fill=blue!30] (tellf) {$f^{*}_{t}$}
            node [circ, right of=gasw, fill=green!30] (gasf) {$f^{*}_{g}$}
            
            node [block, below of=gasw] (newgrid) {$min(med(\Delta x_i))$}
            node at (6,-6) [circ] (xthe) {$x^{the}$}
            
            node [block, right of=starf] (splines) {$C$}
            node [block, right of=tellf] (splinet) {$C$}
            node [block, right of=gasf] (splineg) {$C$}
            
            node [circ, right of=splines, fill=red!30] (fsthe) {$f^{the}_s$}
            node [circ, right of=splinet, fill=blue!30] (ftthe) {$f^{the}_t$}
            node [circ, right of=splineg, fill=green!30] (fgthe) {$f^{the}_g$}
            
            node [circ, right of=fsthe] (mult1) {$\times$}
            node [block, below of=fgthe] (res) {$R(\lambda)$}
            node [block, right of=res] (lsf) {LSF}
            
            node [circ, right of=fgthe, fill=gray!30] (ftot) {$f_{tot}$}
            
            node [circ, right of=ftot] (conv) {$\otimes$}
            node [circ, below of=conv, fill=pink!30] (flsf) {$f_{lsf}$}
        ;
        
        \draw
            node at (1,-10) [circ,name=xdet] {$x_{det}$}
            node [block, right of=xdet] (wavet) {WT}
            node [circ, right of=wavet] (xhat) {$\hat{x}$}
            node [block, right of=xhat] (lanc) {$L$}
            node [circ, right of=lanc] (fexp) {$f_{exp}$}
            
            node [circ, right of=fexp] (nexp) {$N_{exp}$}
            
            node [block, right of=nexp] (snrt) {$SNR_{true}$}
            node [circ, below of=snrt] (noise) {$n$}
            node [circ, below of=nexp] (add) {$+$}
            node [circ, left of=add] (fread) {$N_{read}$}
            node [block, left of=fread] (snrr) {$SNR_{read}$}
            node [circ, left of=snrr] (ferr) {$\sigma_N$}
        ;
            \draw[->](xdet) -- node {}(wavet);
            \draw[->](wavet) -- node {} (xhat);
            \draw[->](xhat) -- node {} (lanc);
            \draw[->](lanc) -- node {} (fexp);
            \draw[->](fexp) -- node {} (nexp);
            \draw[->](nexp) -- node {} (snrt);
            \draw[->](nexp) -- node {} (add);
            \draw[->](snrt) -- node {} (noise);
            \draw[->](noise) -- node {} (add);
            \draw[->](add) -- node {} (fread);
            \draw[->](fread) -- node {} (snrr);
            \draw[->](snrr) -- node {} (ferr);
            
            \draw[->](xthe) to[out=-90,in=100] node {} (lanc);
            \draw[->](flsf) to[out=-90,in=80] node {} (lanc);
            % Joining blocks. 
            % Commands \draw with options like [->] must be written individually
        	\draw[->](star) to[out=30,in=150] node {}(starf);
         	\draw[->](tell) to[out=30,in=150] node {} (tellf);
        	\draw[->](gas) to[out=30,in=150] node {} (gasf);
        	\draw[->](star) -- node {}(starw);
         	\draw[->](tell) -- node {} (tellw);
        	\draw[->](gas) -- node {} (gasw);
            \begin{scope}[on background layer]
                \draw[dotted](starw) -- node {} (newgrid);
             	\draw[dotted](tellw) -- node {} (newgrid);
            	\draw[dotted](gasw) -- node {} (newgrid);
            \end{scope}
            \draw[->](newgrid) -- node {}(xthe);
            
            \draw[->](starf) -- node {} (splines);
         	\draw[->](tellf) -- node {} (splinet);
        	\draw[->](gasf) -- node {} (splineg);
        	
        	\draw[->](starw) to[out=-45,in=-135] node {$+\delta x_i$} (splines);
         	\draw[->](tellw) to[out=-30,in=-150] node {} (splinet);
        	\draw[->](gasw) to[out=-30,in=-150] node {} (splineg);
        	
        	\draw[dotted](xthe) to[out=30,in=-90] node {} (splineg);
        	\draw[dotted](splineg) -- node {} (splinet);
        	\draw[dotted](splinet) -- node {} (splines);
        	
    		\draw[->](splines) -- node {} (fsthe);
         	\draw[->](splinet) -- node {} (ftthe);
        	\draw[->](splineg) -- node {} (fgthe);
        	
        	\draw[->](fsthe) -- node {} (mult1);
         	\draw[->](ftthe) -- node {} (mult1);
        	\draw[->](fgthe) -- node {} (mult1);
        	
        	\draw[->](mult1) -- node {} (ftot);
        	\draw[->](xthe) -- node {} (res);
        	\draw[->](res) -- node {} (lsf);
        	\draw[->](lsf) -- node {} (conv);
        	\draw[->](ftot) -- node {} (conv);
        	\draw[->](conv) -- node {} (flsf);
        	
        	% Adder
        	% Boxing and labelling noise shapers
        	\draw [color=gray,thick](-0.5,-7) rectangle (14,1);
        	\node at (-0.5,1) [above=5mm, right=0mm] {\textsc{Theory}};
        	\draw [color=gray,thick](-0.5,-13) rectangle (14,-9);
        	\node at (-0.5,-9) [above=5mm, right=0mm] {\textsc{Sampling}};
        \end{tikzpicture}
        \caption{Overview of the simulator to be built for WP2. The ``gas'' input handles the (now less common) case of gas-cell-calibrated spectrographs.\label{fig:wp2}}
    \end{figure}

The simulator produced in WP2 will not be used to create toy data, but instead attempt to make data as close as possible to real spectrograph outputs.
It works by combining highest-available resolution theoretical models for emission, transmission, and telluric absorption, and processing these models to mimic a finite-resolution falling on a pixelized detector.
A diagram of this process is presented in \figref{fig:wp2}. To start the simulation, we need a model for the spectral emission of star. We choose the \project{PHOENIX} models \cite{phoenix}, which were generated using 3D models of the photosphere of stars of varying temperature, surface gravity, metalicity, and helium content. We use \text{astropy} \cite{astropy} units to ensure accuracy of units transformations; the simulator keeps track of the physical quantities starting with photon phase-space density at the photosphere.
The user then has to option of adding one or many transmission models to the simulation. In \figref{fig:wp2}, we show that if users want to simulate, say, the Keck \project{HIRes} spectrograph, they can add transmission models for both the atmosphere and the internal gas cell. 

Since all these models for flux and transmission are not  created on the same wavelength grids, they must be interpolated onto a common grid before they can be combined. In order to preserve the information in each model, the simulator creates a new linearly-spaced grid in log wavelength with sufficient resolution (which is very high!).
Then, each model is interpolated unto this new grid using best interpolation methods.
The emission flux grid of the star is the multiplied by each of the transmission grids to get a total flux, at high resolution (that is, before instrument convolution). The total flux is convolved with the point spread function with a kernel that varies with wavelength as per instrument specifications. In the simplest case, this kernel will be a Gaussian with a spread $\sigma = 1/R$ in the log wavelength. But studies into the line spread function show that this width varies over each order in the spectrograph.

Finally the detector samples this theoretic model of flux down to the wavelength grid of the detector. In addition there is a ``WT'' model that implements potential wavelength distortions that occur due to miscalibration of the instrument. These transformations can be caused by variations in room temperature, jitter of the CCD, or gaps in the stitching of the CCD. Future work will be done how to best parameterize these processes, but for now, we just consider this as an arbitrary function that changes the wavelength grid. For the downsampling, we will use Lanczos interpolation to sample the transformed wavelength grid onto the theoretical flux. The signal to noise ratio is then determined at each pixel given an exposure time, which is used to sample noise at each pixel. This is added to the expected flux to produce the readout flux. Then a second grid of signal to noise ratios is generated as the read out signal to noise ratio. Statistically, these two grids will be very similar except for the deepest and lowest count lines, but the goal, after all, is to produce accurate data. All of this along with parameters used at each step of this process is saved to an H5 file.

Modules that we will design and add to the WP2 package include:
\begin{description}
\item[Instrument specifications]
Every instrument has different resolution, sampling, vignetting, noise, and so on. Thus the simulator requires configuration specific to every spectrograph. As part of WP2, we will deliver configurations for the \project{EXPRES} and \project{NEID} spectrographs, as they are two new, bleeding-edge spectrographs and very aligned with the NASA-supported exoplanet community. These configurations can be copied and modified to build accurate configuration specifications for other spectrographs, such as \project{HARPS}.
\item[Stellar variability]
The emission from the star varies over the epochs in a way that is consistent with known stellar variability. Thus the extraction of RV through stellar variability can be tested. This is an important next step in field of EPRVs because stellar variability is the ``tall pole'' in radial velocity error mitigation. Showing exactly how stellar variability conceals RV signals is valuable not only for the discovery of exoplanets, but also for the characterization of the stellar atmosphere. 
\item[2D Simulations]
Future modules for WP2 could also include simulating full 2D images of spectrographs by inverting the flat relative optimal extraction process (see the description of FROE in WP3). These simulated images will be used as test data for WP3. Once again, we can preserve science traceability such that the extraction algorithm hyperparameters can be varied to investigate impacts on extraction and precision of the delivered RVs.
\item[Wavelength distortions]
Shifts, stretching, or bending inside the CCD can be simulated as transformations to the measured wavelengths. This is important for testing wavelength calibration algorithms. Particularly, for testing the impact of unmodeled wavelength calibration issues on RV measurements.
\end{description}
All these modules can be switched on or off one by one, so the user can be determine how exactly the data will be simulated for their particular use-case. 

The deliverables for this project will be open-source, extensible code, configurations for the \project{NEID} and \project{EXPRES} spectrographs, documentation, tutorials for new users, and a paper submitted to the AAS family of journals.

\section{WP3: Information-preserving spectroscopic extraction}

Generally, EPRV proceeds by observations with a two-dimensional detector in a spectrograph focal plane, followed by extraction of a one-dimensional spectrum, or one one-dimensional spectrum per Echelle order.
Fundamentally, \emph{the single most important property of this extraction step is that it preserve the radial-velocity information}.
That is, it is critical that the one-dimensional extracted spectrum contain all of the information about the stellar velocity that was present in the two-dimensional data.
We believe that there is currently no method for this extraction that meets this information-preservation requirement.
In activity WP3 our goal is to \textbf{produce a method and code for one-dimensional spectral extraction that preserves as much of the radial-velocity information as possible}.

For these purposes, ``information'' will be defined as in Box~1: It is the expected inverse variance in a measurement or estimate made with a (justified) maximum-likelihood estimator.
If one observation leads to a higher expected inverse variance than another observation, then the one has higher information content or more information.
Inverse variance is the Fisher information, but it is also the property of the observation that increases linearly with time, when observations are either photon-limited or background-limited.

Of course, technically, in an end-to-end engineering system, there are trades and losses in every component.
So some loss of radial-velocity information at the extraction step is permissible, provided that the pipelines meet requirements overall.
However, \textbf{if there are low-cost or no-cost improvements to pipelines that lead to the preservation and transmission of more radial-velocity information, then these improvements are extremely high value in science-per-dollar terms}.
One way to see this is that a pipeline improvement that leads to (say) a 10-percent improvement in information in the extracted spectrum is equivalent to a telescope or hardware improvement that leads to a 10-percent reduction in exposure times, or a grant that delivers a 10-percent increase in observing time.
It will deliver a squared signal-to-noise that is better by 10~percent on any planet discoveries or characterizations.

It is therefore both an ethical requirement and an engineering requirement that NASA-funded projects be saturating their information bounds, when such saturation can come at very low cost.
In WP3 we will produce a method for information-preserving spectral extraction, a publication describing this in the refereed literature, and open-source code with a reference implementation.
This method and code will be available to be integrated into the NEID pipeline, and the pipelines of any other EPRV spectrographs, globally.

At the present day, there are many different methods and codes for spectral extraction.
In the EPRV community, many of the instrument pipelines use some form of flat-relative optimal extraction (FROE) \cite{froe}.
Briefly, FROE works by comparing the 2D spectral image to the 2D image of a flat-field calibration exposure (usually a spectrum of a featureless blackbody calibration lamp).
Define ``columns'' and ``rows'' such that the 2D image has columns that cut across the spectral trace (or the trace of an echelle order) and rows that go along (or nearly along) the spectral trace.
In FROE, the idea is, at each order, the 2D image of the calibration lamp spectrum is pistoned up and down, with a different piston value at every column, until the pistoned calibration image looks as close as possible to the science exposure.
The extracted one-dimensional spectrum is then the values of these individual-column piston values.
Technically FROE returns the extracted one-dimensional \emph{ratio spectrum} between the science exposure and the flat-field exposure.

The FROE concept is beautiful, simple, and effective, because it uses the calibration data directly to build a forward model of the science data.
That is, the pistoned flat-field 2D image is a generative model for the 2D science image.
That's what makes the method ``optimal'': It is a forward model for the raw data, and it can be compared to the raw data with a justified likelihood function or loss function with a proper noise model.
It is also very simple to implement; it permits a generative model of the raw data without the requirement of a full instrument model for the spectrograph optical image.

However, in its current form, FROE sacrifices a small amount of radial-velocity information when applied to most contemporary spectrographs.
The reason is that the directions on the spectrograph focal plane that are normal to the wavelength direction (or the iso-wavelength surfaces) do not align perfectly with the detector pixel grid.
Indeed, the requirement that these align is an important part of the design of the \textsl{EXPRES} spectrograph \cite{expres}, and led to significant hardware design complexity and cost.
For most spectrographs, this alignment is not perfect, and even when alignment is a design goal, it can't be satisfied to arbitrary accuracy.
Thus there are always alignment issues, between wavelength and the detector grid, and FROE (in its standard form) will sacrifice some spectral resolution (and hence RV precision).

One alternative to FROE, which \emph{does correctly account} for these spectral-direction alignment issues, is SpectroPerfectionism (SP) \cite{sp}.
In this method, the 2D science image is modeled using a complete instrument model, which knows the full ``point-spread function'' (PSF) or probability distribution, given a photon wavelength, for the 2D pixel location at which that photon will be detected.
Like FROE, this model is generative---it extracts by making a forward model of the 2D data---but it is different in that it requires (and benefits from) a full instrument model.
Despite this, \emph{SP also does not completely preserve the RV information} in its current standard form.
The reason is that by using the full PSF in its model, SP builds a model for how an \emph{infinite resolution} 1D spectral input would illuminate the focal plane.
That leads to ringing, which in turn is damped out with a post-model reconvolution of the results.
This reconvolution step is reminiscent of a part of the CLEAN algorithm in radio astronomy \cite{clean}.
But this reconvolution step---which is technically unavoidable given the model structure---removes some RV information.

Technically, SP could be made without the reconvolution step.
However, the reconvolution is necessary to reduce degeneracies and near-degeneracies when models are built at extremely high resolution.
In principle SP might be improved from an EPRV standpoint with a spectral-model regularization that controlled these degeneracies.
However, in WP3 we will build our method as an extension of FROE rather than SP.
One reason for working on FROE is that the method \emph{is currently the method of choice} in the EPRV community.
Another is that it requires very little knowledge about the instrument, far less than is required by SP.

In WP3 we propose to extend FROE to a new method that retains the conceptual and engineering simplicity of FROE, but correctly accounts for the tilts of the wavelength solution across the spectrograph trace.
In our extension of FROE, we replace the concept of having a particular value for the spectrum at each pixel column with the concept of having a non-parametric, continuous function of wavelength be the spectrum.
This non-parametric function can be evaluated at the relevant central wavelength for every pixel in the device; it doesn't require that all pixels in the column be identical in wavelength.
In other respects, FROE remains similar.
The best-fit non-parametric function becomes the extracted spectrum (or the extracted ratio of the science exposure to the flat exposure).

In detail the spectrum that is extracted here is the pixel-convolved, resolution-convolved stellar spectrum. That is, it is precisely the thing that the spectrograph detector measures.
Our method is statistically righteous and plugs up the information leak in the current implementation of FROE.
It also substantially relaxes constraints on spectrograph design, making hardware cheaper.

An open research question, which is part of WP3, overlaps a research question discussed in WP1: How to represent a spectral model? An interpolation of pixel values? A Fourier or wavelet basis? A set of stellar convolved and coadded stellar lines? We don't know the answer to these questions but are leaning towards Fourier and wavelet bases. However, this is part of the research program, and we will deliver an answer in our code, documentation, and refereed papers.

Importantly the FROE extension produced in WP3 is a \emph{generative model for the 2D data}.
That means that it can be run backwards and used to simulate 2D spectrograph images.
That is an explicit part of WP2 (as mentioned above).
Thus several parts of the WP2 and WP3 packages overlap.

Once again, deliverables for WP3 is a method for spectral extraction, open-source code that can be inserted into pipelines, documentation for that code, tutorials for new users, and papers submitted to the AAS family of journals.

\section{Community workshops}

For all three of WP1, WP2, and WP3, code packages will be produced along with documentation and tutorials on how to appropriately use, reimplement, interface with, or extend these programs. However, to build an client base that is invested in furthering this project, we will also plan Community Workshops to educate, build, support, and empower our users and the communities around them. We have seen the value of hands-on, community-building workshops in expolanet science and in stellar science in the past, and we want to capitalize on this and reinforce it.

There will be two Community Workshops held as part of this proposal. These will be in Years 2 and 3 of the proposal timeline (see below).
The Workshops do not appear in the budget of the program, because we have a partnership with an institution with good facilities and resources to support these Workshops.
The letters of support attached to the non-anonymized part of this proposal attest to these commitments.

The goal of these workshops will be not only to to get users familiar with the package, but also to gain insight into what the community would find most useful additions to each package. This will change the priorities of our timeline if the clients have preference on what is most important to the future of EPRV. For one specific example, in WP2, there are decisions to be made about which spectrographs to model first, and how accurately. These priorities will be set by the community at the first Community Workshop.

The most important objective of the Communty Workshops is to \textbf{build community}.
At present, the pipelines for different EPRV projects are disjoint forks of legacy code, developed and maintained by disjoint teams.
There is amazing expertise and capability in those teams, and there is a vibrant intellectual activity represented by those code bases and pipelines.
We want to nucleate and convene this community and create new kinds of collaborations and discussions about hardware, software, and precision measurement to help all projects, now and in the future.
\emph{Our goal is to change the world.}

\section*{Data management plan}

This project will not produce new data per se, but it does produce methods, code, and documents (code documentation and scientific papers). It might also produce model parameters or components of spectral models. All of the code, parameters, models, documentation, and all written documents relating to the project will be maintained in publicly visible version-control repositories on the \project{GitHub} and \project{GitLab} platforms. These repositories can be cloned and reproduced at no cost by any user in the world.

The code and documents will be licensed for re-use by others under permissive licenses (MIT for code, CC-by for documents). This permits re-use, reproducibility, and re-publication by others.

Although we do not expect to deliver large data sets, if we do create any catalog of stellar radial velocities or RV corrections or other per-observation data or meta-data, we will volunteer these to the NASA \project{NEID}, NASA \project{ExoFOP}, and ESO \project{HARPS} Archives (as appropriate) for hosting and preservation. If the Archives are not appropriate for our data products, we will host these data on versioned and mirrored Github and GitLab repositories, both of which accept data sets as large as anything that we could produce here. The same is true for any suites of simulated data we make (in WP2) for any spectrograph.
Once again, everything we produce will be globally available and licensed for arbitrary re-use.

Any cite-able data, code, or documentation will be delivered to \project{Zenodo} for preservation and creation of a lasting and cite-able DOI. All documents and papers will be submitted to the \project{arXiv} and also AAS-family journals for refereeing, long-term preservation, cite-ability, and publication.

\section*{Timeline and work effort}

The table of work effort is shown in \tabref{tab:anon}.
The details are discussed in the Expertise and Resources part of this proposal, including a letter of support from our Collaborator.

Project management will be provided by the PI, including scientific direction and the supervision of testing the code and writing up the scientific papers.
The majority of the coding, documentation, and paper writing will be performed by the Co-I, who is also a PhD student.
All three work packages will have effort in parallel, but with some staging:

\paragraph{Year 1:}
In Year 1, focus will be on WP1 and WP2.
For WP1, the project will begin with duplication of the capabilities of the \project{wobble} method, but with simpler code.
This duplication includes testing on the \project{wobble} test cases \cite{Bedell2019}.
For WP2, the goal in Year 1 will be to find and integrate all inputs (stellar models, telluric models), test their appropriateness, and build and integrate all the components of the simulator.
For the simulator we will also build the configuration package for the NASA \project{NEID} spectrograph.
Documentation and scientific paper-writing will begin on WP1 and WP2 in Year 1.
The first paper on the spectral simulator (WP2) will be finished in year 1, along with sufficient documentation to permit external use of the simulator.

\paragraph{Year 2:}
In Year 2, work on WP3 will start.
This will begin with a reimplementation of FROE and testing on NASA \project{NEID} data.
Decisions will need to be made about how to represent the wavelength solution in two dimensions.
For WP1 and WP2, the goals will be to get the code and documentation into a state that is friendly to other users.
In Year 2 we will host the first Community Workshop, focused on the software we are building for WP1 and WP2.
One output of the first Community Workshop will be configuration packages for other spectrographs (perhaps \project{HARPS} or \project{EXPRES} or \project{HARPS3} or others), with priorities set by community interest.
Another output will be improvements to the WP1 and WP2 documentation and tutorials.
In Year 2 we will complete the first paper on WP1 along with sufficient documentation to permit external use.

\paragraph{Year 3:}
In Year 3, we will complete all three work packages, and release final code versions, final documentation (including tutorials), and submit final papers.
The Year-3 Community Workshop will be focused on WP3 and the joint goals of information-preserving 2D-to-1D spectral extraction methods and the possibility that RV measurements might be made in the 2D data directly.
One output of this second Community Workshop will be improved documentation and interface for the extraction software of WP3, including tutorials.
Another output of this second Community Workshop will be establishment of a community of exoplaneteers who are interested in working close to the metal on spectral data analysis, but across spectrograph teams; this Community Workshop will host pipeline contributors from as many instrument teams as possible.

\paragraph{Summary of deliverables:}
The deliverables from each work package will be open-source code, documentation, tutorials, and submitted papers (to the refereed literature).
In addition to these deliverables, the project will convene two Community Workshops to build communities of spectoscopists working close to the metal on EPRV.

\clearpage{\raggedright %
\bibliographystyle{plain}
\bibliography{xrp}}

\clearpage\pagenumbering{gobble}
\section*{Work effort table}
\begin{table}[h!]
\sffamily\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
role & funded effort & unfunded effort & total effort\\
\hline
\hline
           PI &  1 &  0 &  1 \\
         Co-I & 12 &  0 & 12 \\
 collaborator &  0 &  0 &  0 \\
\hline
\end{tabular}
\vspace{-2ex}
\end{center}
\caption{Work effort (anonymized) in months per year \label{tab:anon} for all three years of the proposal period.}
\end{table}

\clearpage
\markboth{}{\color{gray}\sffamily New tools for extreme-precision spectrographs / Hogg \& Daunt}
\section*{Team expertise}

\paragraph{David W. Hogg:}
PI Hogg is (Full) Professor of Physics and Data Science at New York University. He works across all areas of astrophysics, from cosmology to exoplanets, where data analysis problems are hard and important.
His work in (and related to) exoplanets includes the radial-velocity (RV) measurement method and code \project{wobble} \cite{Bedell2019}, which is very closely related to WP1 of this project, the spectrograph calibration method and code \project{Excalibur} \cite{excalibur}, and the MCMC sampler \project{emcee} \cite{emcee}, which is core technology in the exoplanet community.
He has also worked on exoplanet discovery and characterization \cite{DFMK2, keplersingles} and exoplanet populations inference \cite{myers, exopop}.

The PI has been involved in large hardware and software projects for his entire career, starting with pipelines for the Keck \project{LRIS} instrument (imaging mode) as part of his PhD work, and pipelines for real-time operations of the \project{Sloan Digital Sky Survey} in his postdoc years \cite{hoggpt}.
Since then he has worked on \project{SDSS-3} and \project{SDSS-IV}.
Currently he is involved in the \project{EXPRES} project as a consultant on calibration issues, and is involved in the \project{Terra Hunting Experiment} (with Megan Bedell) as the team contributing to operations and planning from the Flatiron Institute.
He is also working on fiber-robot positioning and calibration for the \project{SDSS-V} project.
He recognizes and believes that this kind of technical work is core to science, important for graduate-student education, and intellectually exciting.

PI Hogg (with Megan Bedell) has been working on information theory in EPRV, and has a paper forthcoming showing which RV measurement methods saturate information-theoretic bounds. This paper is the intellectual underpinning for WP1 of this proposal.
It also gives recommendations for future pipelines.

The PI has a career in building and supporting high-impact software packages for astrophysics, including \project{emcee} \cite{emcee}, \project{The Cannon} \cite{Cannon, Cannon2}, \project{The Joker} \cite{joker}, and \project{Astrometry.net} \cite{an}.
The first and last of these have thousands and hundreds of citations respectively, and are used in a large number of projects across astrophysics.
The group of the PI knows how to interface with and serve technical communities in astrophysics, and loves doing it.

The PI has a strong reputation for open science, performing data analysis and writing in the open, and a strong preference for working in open and public data sets. As mentioned above, the PI is a long-term supporter and maintainer of open-source software systems of wide use in the astronomical community, but he also publishes all code, data, papers, and even proposals on open-access systems for the whole community.

The PI has a part-time appointment at the Flatiron Institute as the Group Leader for Astronomical Data.
This appointment is relevant to the current proposal for two reasons.
The first is that \emph{the Flatiron Institute provides no support for the PhD students} of its group leaders or research scientists.
Thus the current proposal to fund Co-I Matthew Daunt.
The second reason is that the Flatiron Institute provides full financial, space, and catering support to workshops recommended by the Group Leaders; the space, facilities, and administrative support for workshops are all literally world-class.
For this reason, \emph{the workshops that make up part of this proposal are not explicit budget items in the project budget}.
See the letter of support attached to this proposal from the Director of the Flatiron Center for Computational Astrophysics for an explicit statement of commitment.

\paragraph{Matthew Daunt:}
Co-I Daunt is a PhD candidate at New York University.
He is interested in RV measurement, spectroscopic data analysis, and precision measurement.
He has developed a method for extracting sub-frame RV measurements from SDSS-IV APOGEE exposures, to increase by a factor of 8 the time resolution of the spectral time series.
His PhD research is driven by the long-term goal of increasing the information capacity of RV (and other) spectroscopic analyses either by obviating the lossy steps (like 1D spectral extraction, averaging epochs, and so on), or else by fixing them.

\paragraph{Megan Bedell:}
Collaborator Bedell is Associate Research Scientist at the Flatiron Institute Center for Computational Astrophysics.
She is the Project Lead for the Flatiron Institute's contributions to and involvement in the \project{HARPS3} and \project{Terra Hunting Experiment} projects, which are building hardware, software, and observing strategies to find Earth-like planets around Sun-like stars over the next decade.
She is also the lead, designer, developer, and maintainer of the \project{wobble} project \cite{Bedell2019} on which WP1 is based.
Bedell is not a co-I on this proposal, because her position at the Simons Foundation makes her ineligible for funding from NASA (or any other funding agency).
While she is not a co-I on this proposal, she is involved in the projects described in this proposal as a consultant.
She is also a world expert in running workshops and hack weeks, and will help with the organization and hosting of the workshop parts of this proposal, which (as noted above) will be funded by the Flatiron Institute.
Bedell's involvement in the projects described in this proposal is confirmed in a letter of support attached to this proposal.

\paragraph{Workshops:}
PI Hogg and Collaborator Bedell have a history of co-organizing high-impact, hands-on workshops for the astronomical community.
Hogg has been involved in the \project{Astro Hack Week} family of meetings, the \project{Gaia Sprint} family of meetings, and \project{TESS} community workshops.
Bedell ran \project{Telluric Line Hack Week} which was very much in the spirit of the Community Workshops we want to run as part of this project:
It brought together technical people from instrument teams around the world. The discussion was wide-ranging. Talks were informal. And there was plenty of time to work, hack, and start new projects.
Many new projects started, and we learned a huge amount about tellurics and how they are handled and modeled (or masked!).
Many of the above meetings were organized at the Flatiron Institute, where we have world-class facilities, and best-in-the-world staffing for these events.
The Community Workshops are the lowest risk part of this proposal!

\paragraph{Prior NASA support:}
PI Hogg has been supported primarily by NASA for most of his career, going back to his PhD.
He has been supported with \project{HST} and \project{Spitzer} observing grants, with a \project{Hubble Fellowship} and a \project{Long-Term Space Astrophysics} grant at the beginning of his career.
His recent grants have been \project{ADAP} grants to work on spacecraft mission data.
In one way or another, almost all of Hogg's publications can be attributed to prior NASA support.
NASA has been a great supporter of contributions to data analysis and software in astrophysics, and a great supporter of the PI's research.

The only relevant current support for the PI is a subcontract from Bean (Chicago) to build a forward model of the gas cell absorption in a gas-cell-calibrated spectrograph (Keck \project{HIRes}).
This project is related to the current proposal in WP1 (and explains why we'd like to put a gas cell into WP2), but does not relate or overlap in any significant way to the project or work effort or deliverables described in this proposal.

\clearpage
\section*{Biographical sketch --- David W Hogg}

\setlength{\tabcolsep}{0em}
\begin{tabular}{lll}
Center for Cosmology and Particle Physics & \hspace{6em} & \texttt{david.hogg@nyu.edu} \\
Department of Physics                     & & \url{http://cosmo.nyu.edu/hogg/} \\
New York University                       & & 
\end{tabular}

\paragraph{Education}
\begin{list}{}{\hogglist}
\item
PhD 1998, Physics, California Institute of Technology.
\item
SB 1992 (Physics), Massachusetts Institute of Technology.
\end{list}

\paragraph{Current positions}
\begin{list}{}{\hogglist}
\item
Professor of Physics and Data Science, New York University, 2014--.
\item
(part-time) Group Leader, Astronomical Data Group, Flatiron Institute, 2017--.
\end{list}

\paragraph{Recent service}
% reverse chronological by END date.
\begin{list}{}{\hogglist}
\item
\project{Sloan Digital Sky Survey IV} Collaboration Council,
2013--present.
\item
US National Astronomy and Astrophysics Advisory Committee (AAAC)
2014--2017.
\end{list}

\paragraph{Relevant management experience}
%%% reverse order by expxsyiration date
\begin{list}{}{\hogglist}
\item
Advisor for 11 graduated PhD students and 2 current PhD students:
Morad~Masjedi (PhD in 2007);
Dustin~Lang (2009);
Ronin~Wu (2010);
Jo~Bovy (2011);
Adi~Zolotov (2011);
Tao~Jiang (2012);
Fengji~Hou (2014);
Daniel~Foreman-Mackey (2015);
Mohammadjavad~Vakili (2017);
Dun~Wang (2018);
Alex~Malz (2019);
Kate~Storey-Fisher (current);
Matthew D. Daunt (current).
\item
Mentor for postdocs at NYU, the Flatiron Institute, and MPIA.
\item
Member of the Oversight Committee for the \textsc{nasa} \project{Spitzer Space Telescope}.
\item
Member of the Technical Advisory Group for the \project{Sloan Digital Sky Survey V}.
\item
Co-developer and co-maintainer for many open-source code bases,
including:\\
\project{Astrometry.net} (automatic recognition and calibration of arbitrary astronomical imaging);\\
\project{The Joker} (a custom Monte Carlo sampler for exoplanets and binary stars);\\
\project{wobble} (a data-driven model to measure radial velocities at extreme precision).
\item
PI on many past Federally funded projects including these recent
\textsc{nasa} projects:\\
\grantnumber{80NSSC19K0533}{Bean}: 
\textit{Improving the sensitivity of radial velocity spectrographs with
data-driven techniques},
\usd{308,326}, 2019--2021;\\
\grantnumber{NNX16AC70G}{Hogg}:
\textit{Ultra-precise photometry in crowded fields: A self-calibration approach},
\usd{100,000}, 2016--2017;\\
\grantnumber{NNX12AI50G}{Hogg}:
\textit{The Lives and Deaths of Planets and Stars in the Value-Added UV Photon Catalog},
\usd{473,705}, 2012--2017; and\\
\grantnumber{AR-13250}{Hogg}:
\textit{Probabilistic Self-Calibration of the \textsc{wfc3} IR Channel},
\usd{119,988}, 2013--2016.
\end{list}

\hypersetup{linkcolor=black}%
\paragraph{Selected relevant publications}
\begin{list}{}{\hogglist}
\item
Tsalmantza,~P. \& Hogg,~D.~W., 2012,
\doi{10.1088/0004-637X/753/2/122}{A data-driven model for spectra:\ Finding double redshifts in the \project{Sloan Digital Sky Survey}},
\textit{Astrophys.\,J.}\ \textbf{753} 122.
\item
Foreman-Mackey,~D., Montet,~B.~T., Hogg,~D.~W., Morton,~T.~D.,
Wang,~D., \& Sch\"olkopf,~B., 2015,
\doi{10.1088/0004-637X/806/2/215}{A systematic search for transiting planets in the \project{K2} data},
\textit{Astrophys.\,J.}\ \textbf{806} 215.
\item
Ness,~M., Hogg,~D.~W., Rix,~H.-W., Ho,~A.~Y.~Q., \& Zasowski,~G., 2015,
\doi{10.1088/0004-637X/808/1/16}{\project{The~Cannon}: A data-driven
approach to stellar label determination},
\textit{Astrophys.\,J.}\ \textbf{808} 16.
\item
Price-Whelan,~A.~M., Hogg,~D.~W., Foreman-Mackey,~D., \& Rix,~H.-W., 2017,
\doi{10.3847/1538-4357/aa5e50}{The Joker: A Custom Monte Carlo Sampler for Binary-star and Exoplanet Radial Velocity Data},
\textit{Astrophys.\,J.}\ \textbf{837} 20.
\item
Anderson,~L., Hogg~D.~W., Leistedt,~B., Price-Whelan,~A.~M., \& Bovy,~J., 2018,
\doi{10.3847/1538-3881/aad7bf}{Improving \project{Gaia} parallax precision with a data-driven model of stars},
\textit{Astron.\,J.}\ \textbf{156} 145.
\item
Bedell,~M., Hogg,~D.~W., Foreman-Mackey,~D., Montet,~B.~T., \& Luger,~R., 2019,
\doi{10.3847/1538-3881/ab40a7}{\project{Wobble}: A data-driven method for precision radial velocities}\
,
\textit{Astron.\,J.}\ \textbf{158} 164.
\item
  Zhao,~L.~L., Hogg~D.~W., Bedell,~M., \& Fischer,~D.~A., 2021,
  \doi{10.3847/1538-3881/abd105}{\project{Excalibur}: A nonparametric, hierarchical wavelength calibra\
tion method for a precision spectrograph},
  \textit{Astron.\,J.}\ \textbf{161} 80.
\item
  Luger, R., Foreman-Mackey,~D., Hedges,~C., \& Hogg,~D.~W., 2021,
  \doi{10.3847/1538-3881/abfdb8}{Mapping stellar surfaces. I. Degeneracies in the rotational light-cur\
ve problem},
  \textit{Astron.\,J.}\ \textbf{162} 123.
\end{list}

\paragraph{Other important publications}
\begin{list}{}{\hogglist}
\item
Hogg,~D.~W. \etal, 2004,
\doi{10.1086/381749}{The dependence on environment of the color--magnitude relation of galaxies},
\textit{Astrophys.\,J.\,Lett.}\ \textbf{601} L29--L32.
\item
Willman,~B., Blanton,~M.~R., West,~A.~A., Dalcanton,~J.~J, Hogg,~D.~W., Schneider,~D.~P., Wherry,~N., Yanny,~B., \& Brinkmann,~J., 2005,
\doi{10.1086/430214}{A new Milky Way companion:\ Unusual globular cluster or extreme dwarf satellite?},
\textit{Astron.\,J.}\ \textbf{129} 2692--2700.
\item
Eisenstein,~D.~J., Zehavi,~I., Hogg,~D.~W., \etal, 2005,
\doi{10.1086/466512}{Detection of the baryon acoustic peak in the large-scale correlation function of \project{Sloan Digital Sky Survey} Luminous Red Galaxies},
\textit{Astrophys.\,J.}\ \textbf{633} 560--574.
\item
Lang,~D., Hogg,~D.~W., Mierle,~K., Blanton,~M., \& Roweis,~S., 2010,
\doi{10.1088/0004-6256/139/5/1782}{\project{Astrometry.net}:\ Blind astrometric calibration of arbitrary astronomical images},
\textit{Astron.\,J.}\ \textbf{139} 1782--1800.
\item
Bovy,~J., Hennawi,~J.~F., Hogg,~D.~W., \etal, 2011,
\doi{10.1088/0004-637X/729/2/141}{Think outside the color-box:\ Probabilistic target selection and the \textsc{sdss-xdqso} quasar targeting catalog},
\textit{Astrophys.\,J.}\ \textbf{729} 141.
\item
Oppenheimer,~B.~R. \etal, 2013,
\doi{10.1088/0004-637X/768/1/24}{Reconnaissance of the HR~8799 exosolar system.\ I.\ Near-infrared spectroscopy},
\textit{Astrophys.\,J.}\ \textbf{768} 24.
\item
Foreman-Mackey,~D., Hogg,~D.~W., Lang,~D., \& Goodman,~J., 2013,
\doi{10.1086/670067}{\project{emcee}:\ The MCMC Hammer},
\textit{Pubs.\,Astr.\,Soc.\,Pac.}\ \textbf{125} 306--312.
\label{rpcount}\end{list}

\setlength{\tabcolsep}{0.5em}
\clearpage
\section*{Biographical sketch --- Matthew D. Daunt}

Graduate School of Arts and Science, New York University \\
e-mail: \href{mailto:mdd423@nyu.edu}{mdd423@nyu.edu} \quad tel: +1-805-791-5965

\paragraph{Professional Preparation:}
\begin{itemize}
    \item Northern Arizona University, Flagstaff, AZ;
        Mathematics;
        B.S., 2018
    \item Northern Arizona University, Flagstaff, AZ;
        Physics;
        B.S., 2018
\item Northern Arizona University, Flagstaff, AZ;
        Research Associate, 2018--2019
    \item New York University, New York, NY;
        PhD Candidate, 2019--present
        \end{itemize}

\paragraph{Appointments:}

\begin{itemize}
    \item 2018--2019:
        \textbf{Research Associate},
        Northern Arizona University,
        Flagstaff, AZ
    \item 2017--2018:
        \textbf{Undergraduate Researcher},
        Northern Arizona University,
        Flagstaff, AZ
    \item 2015--2018:
        \textbf{Lead Physics and Math Tutor},
        Northern Arizona University,
        Flagstaff, AZ
\end{itemize}

\paragraph{Synergistic Activities:}

\begin{itemize}
    \item \textbf{Conference service:}
        (i) Poster Presenter: ``The Influence of Many-body effects on Light-Matter Interactions in Semiconductor Nanostructures'', 2018 National Arizona Planetary Science Association, Flagstaff, AZ,
        October 14, 2018.
        (ii) Organizer for technical session: ``Othello Distribution'', 2018 Southwestern Undergraduate Mathematics Research Conference, Albuquerque, NM,
        April 14, 2018.
    \item \textbf{Mentoring:}
        Lead Math and Physics Tutor at Northern Arizona University,
        Flagstaff, AZ.
    \item \textbf{Teaching:}
        (i) Assisted in classes as Research Associate, Northern Arizona
        University, 2018--2019.
        (ii) Taught labs and recitation as Graduate Student both online and in-person, New York University, 2020--2021.
\end{itemize}

\paragraph{Collaborators \& Other Affiliations:}

\begin{itemize}
    \item \textbf{Collaborators and Co-Editors:}
        \emph{New York University}:
            David W. Hogg,
        \emph{Northern Arizona University}:
            In\'es Monta\~no;
            Roy St Laurent
    \item \textbf{Graduate Advisers:}
        \emph{New York University}:
            David W. Hogg;
            Maryam Modjaz;
            Michael Blanton;
            Marc Gershow
\end{itemize}

\clearpage
\section*{Work effort table}

\begin{table}[h!]
\sffamily\begin{center}
\begin{tabular}{|r|l|c|c|c|}
\hline
name & role & funded effort & unfunded effort & total effort\\
\hline
\hline
    Hogg &           PI &  1 &  0 &  1 \\
   Daunt &         Co-I & 12 &  0 & 12 \\
  Bedell & collaborator &  0 &  0 &  0 \\
\hline
\end{tabular}
\vspace{-2ex}
\end{center}
\caption{Work effort in months per year, for all three years of the proposal period.\label{tab:work}
Bedell's effort is shown as zero because her employment at the Simons Foundation does not permit external funding from any agency.
Her commitment to the project is stated in a letter attached to this proposal.}
\end{table}

\clearpage
\section*{Facilities and equipment}

The NYU Department of Physics is part of the NYU Faculty of Arts \&
Science and includes the Center for Cosmology and Particle Physics,
the Center for Soft Matter Research, and the Center for Quantum Phenomena.
A wide variety of sponsored
research activities take place in the Department.  These range from
the theoretical to the pragmatic, and include a broad spectrum of
interactions with such disciplines as biology, medicine, chemistry,
applied mathematics, and computer science.

In addition to benefitting from this research activity directly and
indirectly, as a member of the Department, the PI receives through the
Department staff support for clerical work, post-award grant support,
and for computing (expanded upon below).

\paragraph{Computing Equipment:}
The astrophysics group at NYU maintains some high-performance computers
for the PI, and substantial storage machines.
The latter contain more than 100~Tb of disk space, most of which is filled
with astrophysical imaging data.  The Physics Department maintains a
state-of-the-art controlled-environment computer room that houses and
protects the computers that will be used in this project.

\paragraph{General computer resources:}
The University has hired a Director of Scientific Computing for the
NYU Center for Cosmology and Particle Physics (the PI's home).  His
responsibilities include management of the cluster and the data
servers, and overall management and supervision of the Center's
computer system.  He will oversee the computer hardware used in this
project.

A variety of computing resources are available within the Physics
Department, including UNIX workstations, laptops, laser color
printers, etc.  The Department runs a network of several hundred
desktop workstations, file servers, in addition to the multi-processor
computational server mentioned above.

Computational needs are also supported through the University's
Academic Computing Services, a unit of the NYU-wide Information
Technology Services offering an additional wide range of computational
resources in support of research and instruction.  These include a
variety of computing platforms, including several high-performance
multi-CPU systems, and scientific software.  Consultants are available
to assist in the use of these resources.

Finally, the NYU Center for Data Science (where the PI has an appointment)
maintains some specialized high-performance machines, including a GPU
cluster for machine-learning
applications. These resources could be relevant to the proposed project,
especially as some development might be optimized for GPUs.

\paragraph{Office space:}
The offices of the PI and Co-I will be located in the NYU Physics
Department.

\paragraph{Library resources:}
In addition to an enormous book collection, the NYU libraries hold
current subscriptions to hundreds of hardcopy and electronic journals.
Libraries provides access to all journals relevant to this project and such
databases as MathSciNet, the Web of Science (science citation index),
and the ACM Digital Library. They also provide resources to support
researchers with preservation of data, code, and models.

\paragraph{Meeting spaces:}
The Center for Cosmology and Particle Physics, the NYU Libraries, the
NYU Center for Data Science, and the NYU Kimmel Center for Student
Life all have spaces that are available to the PI for meetings and
workshops.

\paragraph{Experimental and hardware facilities:}
The Department has large experimental facilities, including machining,
imaging, and clean facilities, but none of these are directly relevant
to this project, except insofar as they are part of the rich
intellectual and research atmosphere.

\end{document}
